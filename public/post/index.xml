<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Alex Knows Data</title>
    <link>/post/</link>
    <description>Recent content in Posts on Alex Knows Data</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 22 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Test</title>
      <link>/post/2023-01-22-test/</link>
      <pubDate>Sun, 22 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-22-test/</guid>
      <description>This my newest post.</description>
    </item>
    
    <item>
      <title>20 New Finedust Sensors</title>
      <link>/post/zon-airrohr/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/zon-airrohr/</guid>
      <description>Hiiii. Together with my colleagues at Zeit Online we build 20 new finedust sensors for the luftdaten.info project. I ordered most parts of the sensors via AliExpress so all the participants only have to pay 35 € for their sensor.
I prepared some slides for the workshop. You can find them here.</description>
    </item>
    
    <item>
      <title>Generative art with #rstats</title>
      <link>/post/generative-art/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/generative-art/</guid>
      <description>Playing around with aschinchon strange attractor algorithm. The original blog post can be found here. My version of the code is on my github. The code to create the artworks is embedded in a twitter bot which automatically post images with different parameters every hour.</description>
    </item>
    
    <item>
      <title>Airrohr Finedust Data Analysis</title>
      <link>/post/finedust/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/finedust/</guid>
      <description>In einem Twitter thread habe ich über DIY Feinstaubsensoren berichtet. Hier der Inhalt:
Laut airrohr Daten überschritten viele Hamburger Sensoren in 2018 regelmäßig den zulässigen PM10 Tagesmittelwert von 50 µg/m³. Beispiel: Am 23.11.18 (KW 47) überschritten 182 von 205 Sensoren (89%) den Grenzwert.
Laut EU-Richtlinie sind max. 35 Grenzwertüberschreitungen pro Sensor und Jahr zulässig. Laut airrohr Daten überschritten in 2018 dennoch 43 Sensoren den Grenzwert häufiger als 35 Mal.</description>
    </item>
    
    <item>
      <title>Twitter Bot Open Data Hamburg</title>
      <link>/post/opendata/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/opendata/</guid>
      <description>I have build a twitter bot that tweets everytime a new dataset is being uploaded on the Open Data Portal of Hamburg. You can find the code behind the bot on my Github. It is sheduled via a cronjob on a virtual machine on Google Cloud Platform.</description>
    </item>
    
    <item>
      <title>Data journalism with Spiegel Online</title>
      <link>/post/ddj/</link>
      <pubDate>Tue, 11 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ddj/</guid>
      <description> Between August and September 2018 I had the chance to support some data journalism projects at Spiegel Online as part of my Google Fellowship. Here is a list of things I have worked on during my fellowship:
Blackbox Schufa Deutschland spricht Auswärtstorregel </description>
    </item>
    
    <item>
      <title>#nr18 Twitter Analysis</title>
      <link>/post/nr18/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/nr18/</guid>
      <description>I took my oldschool-ish twitter analysis script and gave it a try for the hastag #nr18. nr18 was the Netzwerk Recherche’s annual conference 2018 which took place on Friday and Saturday 29-30th June at NDR (Northern German Broadcasting) in Hamburg. netzwerk recherche e.V. (nr) is a registered not-for-profit association of journalists founded in 2001. They also had a track on data journalism. Find more info here: https://netzwerkrecherche.org.
As always, you can find the code for all visualizations on my Github.</description>
    </item>
    
    <item>
      <title>Building a recommendation engine in R</title>
      <link>/post/reco-engine/</link>
      <pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/reco-engine/</guid>
      <description>A product based recommendation engine with the recommenerlab R-package. On my Github you will find the R code to create product recommendations on a test data set (see product_views.csv). The data file consists of different website visiors with unique user ids and their views of products in an online shop. This is a made up test data file to put the recommendation algorithm into work. The recommenderlab package provides an infrastructure to test and develop recommender algorithms.</description>
    </item>
    
    <item>
      <title>Marketing Attribution</title>
      <link>/post/attribution/</link>
      <pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/attribution/</guid>
      <description>In the world of e-commerce a customer has often seen more than just one marketing channel before they buy a product. We call this a customer journey. Marketing attribution has the goal to find out the importance of each channel over all customers. This information can then be used to optimize your marketing strategy and allocate your budget perfectly but also gives you valuable insights into your customers. There are a lot of different models to allocate your conversions (or sales) to the different marketing channels.</description>
    </item>
    
    <item>
      <title>Simple but nifty cohorts in R</title>
      <link>/post/cohorts/</link>
      <pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/cohorts/</guid>
      <description>Cohorts are always a great way to split a group into segments and get a deeper view of what ever you looking at. Imagine you have an online shop and would like to know how your user retention has developed over the last view weeks.
The diagram above basically shows the retention rate of fifteen different groups. For example about 25 percent of the people from cohort one came back to visit our online shop 15 weeks after their first visit.</description>
    </item>
    
    <item>
      <title>Visualizing Clustering Results in R</title>
      <link>/post/clustering/</link>
      <pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/clustering/</guid>
      <description>Recently I thought about how to visualize the result of a cluster analysis. I do not mean the visualization of the clusters itself but the results in terms of content and variable description – something you could give away to someone who does not understand the mechanics of cluster algorithms and just want to see a description of the resulting clusters. I came up with a ggplot solution which looks like the following.</description>
    </item>
    
    <item>
      <title>Bike Sharing in Hamburg</title>
      <link>/post/stadtrad/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/stadtrad/</guid>
      <description>Read an article on the usage of open data for bicycle traffic planning on my Medium (in German).
Some important information on this project: Data: http://data.deutschebahn.com/dataset/data-call-a-bike Use the map here: https://alexkruse.shinyapps.io/stadtrad/ I created a Poster for useR 2017 Poster Session and done a workshop for OpenStreetMap. My interactive map shows the bike sharing usage of StadtRAD, the bike sharing system in Hamburg – Germany. The data is available on the open data platform from Deutsche Bahn, the public railway company in Germany.</description>
    </item>
    
    <item>
      <title>Coding Da Vinci</title>
      <link>/post/cdv/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/cdv/</guid>
      <description>Interaktive Karte für Kultur-Hackathon CodingDaVinci
Beschreibung des Projekts: Kulturelle Bildung ist ein unverzichtbarer Bestandteil unseres Bildungssystems, da sie für die Würde des Menschen und für die freie Entwicklung seiner Persönlichkeit unentbehrlich ist (Art. 22, UN-Menschenrechtscharta). Ein wichtiger Baustein ist hierbei der Schulausflug ins Museum, um Schülerinnen und Schüler an Kunst und Kultur heranzuführen sowie deren Persönlichkeitsbildung und gesellschaftliche Kompetenz mittels kultureller Bildung zu stärken. Die interaktive Karte „Hamburger Schüler im Museum“ ermöglicht dem Nutzer folgender Frage nachzugehen: In welchen Gegenden von Hamburg machen die Schulen häufig einen Ausflug ins Museum und in welchen Stadtteilen gibt es Nachholbedarf?</description>
    </item>
    
    <item>
      <title>Coding Dürer</title>
      <link>/post/coding-d%C3%BCrer/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/coding-d%C3%BCrer/</guid>
      <description>Recently I took part at codingdurer, a five days international and interdisciplinary hackathon for art history and information science. The goal of this hackathon is to bring art historians and information scientists together to work on data. It is kind of an extension to the cultural hackathon CodingDaVinci where I participated in the past. I also wrote an article about CDV on this blog.
At CodingDurer we developed a Shiny App to explore the genre of church interior paintings developed in the Netherlands in the middle of the 17th century.</description>
    </item>
    
    <item>
      <title>Google Takeout</title>
      <link>/post/takeout/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/takeout/</guid>
      <description>R related Google Search History over time (animated by month with gganimate). Network done with ggnetwork. To get your own Google Search History you just need to go here https://takeout.google.com/settings/takeout and download your data The analysis takes only words into account that occured more than twenty times in my Google search history. And it only take connection between words into account that occured more than once. Without these filters there would be too much (uninteressting) data to plot.</description>
    </item>
    
    <item>
      <title>Ten Decades of Street Tree Planting in Hamburg</title>
      <link>/post/streettree/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/streettree/</guid>
      <description>Hamburg is doing great with its open data platform where you can find a lot of cool data sets to play with. I gave it shot with ten decades of street tree planting data and made an animated poster-style visualization. It would be awesome not only to get data on tree plantings but also felling. I am in contact with the authorities but it seems quite hard to get on the data.</description>
    </item>
    
    <item>
      <title>Top 16% Solution to Kaggle’s Product Classification Challenge</title>
      <link>/post/kaggle/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/kaggle/</guid>
      <description>Kaggle is a platform for predictive modelling and analytics competitions on which companies and researchers post their data and statisticians and data miners from all over the world compete to produce the best models. As of May 2016, Kaggle had over 536,000 registered users, or Kagglers. The community spans 194 countries. It is the largest and most diverse data community in the world (Wikipedia).
One of my first Kaggle competitions was the OTTO product classification challange.</description>
    </item>
    
    <item>
      <title>Twitter Analysis of Coding Dürer</title>
      <link>/post/twitter/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/twitter/</guid>
      <description>Recently I took part at Coding Durer, a five days international and interdisciplinary hackathon for art history and information science. The goal of this hackathon is to bring art historians and information scientists together to work on data. It is kind of an extension to the cultural hackathon CodingDaVinci where I participated in the past. I have used the twitter API to get all tweets between 13.03. and 19.03. for the hashtag #codingdurer.</description>
    </item>
    
    <item>
      <title>What&#39;s cooking?</title>
      <link>/post/yummly/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/yummly/</guid>
      <description>Kaggle is a platform for predictive modelling and analytics competitions on which companies and researchers post their data and statisticians and data miners from all over the world compete to produce the best models. As of May 2016, Kaggle had over 536,000 registered users, or Kagglers. The community spans 194 countries. It is the largest and most diverse data community in the world (Wikipedia).
One of the most interesting data sets I found on Kaggle was within the What’s Cooking challenge.</description>
    </item>
    
    <item>
      <title>Marathon Hamburg</title>
      <link>/post/marathon-hamburg/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/marathon-hamburg/</guid>
      <description>I came across this cool piece of data vizualization from the folks of Berliner Morgenpost. I thought it would be nice to re-create the whole visualization for the Hamburg Marathon 2018. I came up with a complete R-based solution. The animated visualiaztion itself is made out of multiple ggplot outputs.
I also analyzed the gender differences at the Hamburg Marathon 2018. I would say it is a proper misuse of error bars :) It is the same thing here, I just put together multiple ggplot outputs into a gif.</description>
    </item>
    
    <item>
      <title>#airrohr</title>
      <link>/post/airrohr/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/airrohr/</guid>
      <description>I installed my own open source fine dust sensor on my balcony in September 2017. I build this sensor as part of a bigger citizen science project called luftdaten.info. The data from all fine dust sensors is also available here. By March 2018 I was wondering how many fine dust sensors we have in Hamburg so I created an animated visualization of the project growth in Hamburg. It’s all made with #rstats.</description>
    </item>
    
    <item>
      <title>About me</title>
      <link>/post/about/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/about/</guid>
      <description>Hi, my name is Alexander Kruse and I live in Hamburg-Germany. I do data analysis and visualizations.
Personal Information: Name: Alexander Kruse Date of birth: 03.05.1988, Hamburg-Germany Mail: kruse-alex [at] outlook.de Twitter: krusealex2013 Kaggle: alexkruse Github: kruse-alex Talks: Hamburg R User Group, 01.08.2017, Lightning Talk, Visualization of usage of bike sharing network in Hamburg (StadtRAD) OpenStreetMap, 21.07.2017, Talk, Analysing and visualising bike sharing usage in Hamburg useR 2017, 04.-07.07.2017, Poster Session, Bike sharing usage with Shiny and Leaflet R-Kenntnis-Tage 2016, 2.</description>
    </item>
    
  </channel>
</rss>
